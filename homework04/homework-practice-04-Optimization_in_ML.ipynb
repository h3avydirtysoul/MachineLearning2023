{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce332720",
   "metadata": {},
   "source": [
    "# Методы оптимизации в машинном обучении\n",
    "\n",
    "### О задании\n",
    "\n",
    "Настоящее практическое задание посвящено методу градиентного спуска и методу Ньютона. Задание состоит из пяти разделов. В каждом разделе вам предлагается выполнить несколько заданий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943722e",
   "metadata": {},
   "source": [
    "# 1 Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262b83a",
   "metadata": {},
   "source": [
    "## 1.1 Методы спуска: Общая концепция\n",
    "\n",
    "Рассматриваем задачу гладкой безусловной оптимизации:\n",
    "$$\\underset{x∈R^n}{min} f(x).$$\n",
    "\n",
    "Методы спуска итеративно строят последовательность точек(xk)∞k=0изRnпо правилу\n",
    "$$x_{k+1}=x_k+α_kd_k.$$\n",
    "\n",
    "Число k= 0, 1 ,... называется номером итерации метода. Скаляр $α_k ≥ 0$ называется длиной шага, а вектор $d_k∈R^n$ называется направлением поиска. В методах спуска требуется, чтобы направление поиска $d_k$ являлось направлением спуска для функции f в точке $x_k$, т. е. удовлетворяло нервенству.\n",
    "\n",
    "$$〈∇f(x_k),d_k〉< 0. $$\n",
    "\n",
    "В этом случае можно гарантировать, что для всех достаточно маленьких $α_k$ значение функции f в новой точкеx k+1 уменьшится:\n",
    "$$f(x_{k+1})< f(x_k).$$\n",
    "Общая схема метода спуска приведена ниже:\n",
    "\n",
    "**Алгоритм 1** Общая схема метода спуска  \n",
    "**Вход:** Начальная точка $x_0$; максимальное число итераций $K$.  \n",
    "1:**for k ← 0 to K do**  \n",
    "2:&nbsp;&nbsp;&nbsp;&nbsp;*(Вызов оракула)* Вычислить $f(x_k), ∇f(x_k)$ и пр.  \n",
    "3:&nbsp;&nbsp;&nbsp;&nbsp;*(Критерий остановки)* Если выполнен критерий остановки, то выход.  \n",
    "4:&nbsp;&nbsp;&nbsp;&nbsp;*(Вычисление направления)* Вычислить направление спуска $d_k$.  \n",
    "5:&nbsp;&nbsp;&nbsp;&nbsp;*(Линейный поиск)* Найти подходящую длину шага $α_k$.  \n",
    "6:&nbsp;&nbsp;&nbsp;&nbsp;*(Обновление)* $x_{k+1} ← x_k + α_k d_k$.  \n",
    "7:**end for**  \n",
    "**Выход:** Последняя вычисленная точка $x_k$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831bc55",
   "metadata": {},
   "source": [
    "## 1.2 Критерий остановки\n",
    "\n",
    "Идеальным критерием остановки в методе является проверка условия $f(x_k)−f*< \\tilde{ε}$, где $f*$ - минимальное значение функции $f$, а $\\tilde{ε} > ̃0$ - заданная точность. Такой критерий целесообразно использовать, если оптимальное значение функции $f$ известно. К сожалению, зачастую это не так, и поэтому нужно использовать другой критерий. Наиболее популярным является критерий, основанный на норме градиента: $‖∇f(x_k)‖^2_2 <\\tilde{ε}$. Квадрат здесь ставят за тем, что для \"хороших\" функций невязка по функции $f(x_k)−f*$ имеет тот же порядок, что и $‖∇f(x_k)‖^2_2$ , а не $‖∇f(x_k)‖_2$ (например, это верно для сильно-выпуклых функций с липшицевым градиентом.); например, если $‖∇f(x_k)‖_2 ∼ 10^{−5}$, то $f(x_k)−f* ∼ 10^{−10}$. Наконец, для того, чтобы критерий не зависел от того, измеряется ли функция $f$ в \"метрах\" или в \"километрах\" (т. е. не изменялся при переходе от функции $f$ к функции $tf$, где $t > 0$), то имеет смысл использовать следующий относительный вариант критерия:\n",
    "$$ ‖∇f(x_k)‖^2_2 ≤ ε‖∇f(x_0)‖^2_2 \\tag{1.1},$$\n",
    "где $ε∈(0,1)$ - заданная относительнаяточность. Таким образом, критерий остановки (1.1) гарантирует, что метод уменьшит начальную невязку $‖∇f(x_0)‖_2$ в $ε^{−1}$ раз. В этом задании Вам нужно будет во всех методах использовать критерий остановки (1.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758836a",
   "metadata": {},
   "source": [
    "## 1.3 Линейный поиск\n",
    "\n",
    "Рассматривается функция\n",
    "$$φ_k(α) := f(x_k+αd_k).$$\n",
    "Заметим, что\n",
    "$$φ′_k(α) =〈∇f(x_k+αd_k), d_k〉.$$\n",
    "\n",
    "Поскольку $d_k$ является направлением спуска, то $φ′(0) =〈∇f(x_k),d_k〉< 0$.\n",
    "Условием Армихо для $α$ называется выполение следующего неравенства:\n",
    "\n",
    "$$φ_k(α) ≤ φ_k(0) + c_1 αφ′_k(0),$$\n",
    "где $c_1 ∈(0, 0.5)$ - некоторая константа.\n",
    "Для поиска точкиα, удовлетворяющей условию Армихо, обычно используют следующую процедуру - метод дробления шага (бэктрекинг):\n",
    "\n",
    "**Алгоритм 2** Метод дробления шага  \n",
    "**Вход:** Функция $φ_k:R_+ → R$. Начальная точка: $α^{(0)}_k$.  \n",
    "1:$α ← α^{(0)}_k.$  \n",
    "2:**while** $φ_k(α) > φ(0) + cαφ′_k(0)$ **do**  \n",
    "3:&nbsp;&nbsp;&nbsp;&nbsp;$α ← α/2$.  \n",
    "4:**end while**\n",
    "**Выход:** $α$  \n",
    "\n",
    "\"Адаптивный\" метод подбора шага запоминает величину $α_k$, найденную на текущей итерации и\n",
    "на следующей итерации начинает процедуру дробления $сα(0)_{k+1}:= 2α_k$. Исключение здесь составляют ньютоновские и квазиньютоновские методы - в этих методах процедуру дробления шага всегда нужно начинать $сα(0)_k := 1$.\n",
    "\n",
    "**Сильные условия Вульфа:**\n",
    "$$φ_k(α) ≤ φ(0) + c_1 α φ′_k(0)$$\n",
    "$$|φ′_k(α)|≤ c_2 |φ′_k(0)|$$\n",
    "\n",
    "Здесь $c_1 ∈ (0, 0.5), c_2 ∈ (c_1, 1)$.  \n",
    "Самостоятельно реализовывать схему для сильных условий Вульфа не нужно. Используйте биб-\n",
    "лиотечную реализацию (функция `scalar_search_wolfe2` из модуля `scipy.optimize.linesearch`). В\n",
    "\n",
    "ней начальная длина шага $α^{(0)}_k$ автоматически выбирается равной 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1bbf4",
   "metadata": {},
   "source": [
    "## 1.4 Градиентный спуск\n",
    "\n",
    "Градиентный спуск:\n",
    "$$x_{k+1}=x_k−αk∇f(x_k)$$\n",
    "Можно рассматривать как метод спуска, в котором направление поиска $d_k$ равно антиградиенту\n",
    "$−∇f(x_k)$. Длина шага $α_k$ выбирается с помощью линейного поиска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afeb4d7",
   "metadata": {},
   "source": [
    "## 1.5 Метод Ньютона\n",
    "\n",
    "Метод Ньютона:\n",
    "$$x_{k+1}=x_k−α_k[∇^2 f(x_k)]^{-1} ∇f(x_k).$$\n",
    "Для метода Ньютона очень важно использовать единичный шаг $α_k = 1$, чтобы обеспечить локальную квадратичную сходимость. Поэтому в алгоритмах линейного поиска нужно всегда первым делом\n",
    "пробовать единичный шаг. Теория гарантирует, что в зоне квадратичной сходимости метода Ньютона\n",
    "единичный шаг будет удовлетворять условиям Армихо/Вульфа, и поэтому автоматически будет приниматься. Если единичный шаг не удовлетворяет условиям Армихо/Вульфа, то алгоритмы линейного\n",
    "поиска его уменьшат и, тем самым, обеспечат глобальную сходимость метода Ньютона.  \n",
    "\n",
    "Вычисление Ньютоновского направления $d_k=−[∇^2 f(x_k)]^{-1} ∇f(x_k)$ эквивалентно решению линей-\n",
    "ной системы уравнений:\n",
    "$$∇^2 f(x_k)d_k=−∇f(x_k).$$\n",
    "Если гессиан - положительно определённая матрица: $∇^2 f(x_k) \\succ 0$ , то предпочтительным методом решения такой системы является разложение Холецкого, которое также, как и метод Гаусса, работает за $O(n^3)$, но является вычислительно более эффективным. Если матрица системы не является положительно определённой, то метод Холецкого сможет обнаружить и сообщить об этом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed864b",
   "metadata": {},
   "source": [
    "<!-- ## 1.6 (Бонусная часть) Оптимизация вычислений\n",
    "\n",
    "```\n",
    "Рассмотрим случайf(x) =ψ(Ax).\n",
    "В этом случае\n",
    "∇f(x) =AT∇ψ(Ax).\n",
    "```\n",
    "Для линейного поиска:\n",
    "\n",
    "```\n",
    "φ(α) =ψ(Axk+αAdk), φ′(α) =〈∇ψ(Axk+αAdk),Adk〉.\n",
    "```\n",
    "Алгоритм 3Общая схема метода спуска дляf(x) =ψ(Ax)\n",
    "\n",
    "```\n",
    "1:fork← 0 toK− 1 do\n",
    "2: (Вызов оракула)Вычислитьf(xk) =ψ(Axk),∇f(xk) =AT∇ψ(Axk)и пр.\n",
    "3: (Вычисление направления)Вычислить направление спускаdk.\n",
    "4: (Линейный поиск)Найти подходящую длину шагаαk:\n",
    "5: Вычислитьφ(0) =ψ(Axk),φ′(0) =〈∇ψ(Axk),Adk〉.\n",
    "6: Вычислитьφ( ̄α 1 ) =ψ(Axk+ ̄α 1 Adk),φ′( ̄α 1 ) =〈∇ψ(Axk+ ̄α 1 Adk),Adk〉.\n",
    "7: ...\n",
    "8: Вычислитьφ( ̄αs) =ψ(Axk+ ̄αsAdk),φ′( ̄αs) =〈∇ψ(Axk+ ̄αsAdk),Adk〉.\n",
    "9: (Обновление)xk+1←xk+ ̄αsdk.. Axk+1=Axk+ ̄αsAdk\n",
    "10:end for\n",
    "```\n",
    "Таким образом, в хорошей реализации должно быть в среднем лишь дваматрично-векторных про-\n",
    "изведения: одно \u0016 чтобы вычислить градиентAT∇ψ(Axk), второе \u0016 чтобы вычислитьAdk. Сами\n",
    "матрично-векторные произведенияAxkможно пересчитывать, используяAdk.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb48c6",
   "metadata": {},
   "source": [
    "# 2 Модели\n",
    "\n",
    "## 2.1 Двухклассовая логистическая регрессия\n",
    "\n",
    "Логистическая регрессия является стандартной моделью в задачах классификации. Для простоты\n",
    "рассмотрим лишь случай бинарной классификации. Неформально задача формулируется следующим\n",
    "образом. Имеется обучающая выборка $((ai, bi))^m_{i=1}$, состоящая изmвекторов $a_i ∈ R^n$ (называемых признаками) и соответствующих им чисел $b_i ∈ {−1, 1}$ (называемых классами). Нужно построить алгоритм $b(·)$, который для произвольного нового вектора признаков $a$ автоматически определит его класс $b(a)∈{−1, 1}$.  \n",
    "\n",
    "В модели логистической регрессии определение класса выполняется по знаку линейной комбинации\n",
    "компонент вектораaс некоторыми фиксированными коэффициентами $x∈R^n$:\n",
    "$$b(a) := sign(〈a,x〉).$$\n",
    "\n",
    "Коэффициенты $x$ являются параметрами модели и настраиваются с помощью решения следующей\n",
    "оптимизационной задачи:\n",
    "$$\\underset{x∈R^n}{min} \\left( \\frac{1}{m}\\sum_{i=1}^m ln(1 + exp(−b_i〈a_i, x〉)) + \\frac{λ}{2}‖x‖^2_2 \\right) $$\n",
    "где $λ > 0$ - коэффициент регуляризации (параметр модели).\n",
    "\n",
    "## 2.2 Разностная проверка градиента и гессиана\n",
    "Проверить правильность реализации подсчета градиента можно с помощью конечных разностей:\n",
    "$$[∇f(x)]_i ≈ \\frac{f(x+ε_1 e_i)−f(x)}{ε_1},$$\n",
    "где $e_i:= (0,..., 0 , 1 , 0 ,...,0)$ - i-й базисный орт, а ε_1 - достаточно маленькое положительное число: $ε_1 ∼ \\sqrt{ε_{mach}}$, где $ε_{mach}$ - машинная точность ($≈ 10 ^{-16}$ для типа `double`).\n",
    "\n",
    "Вторые производные:\n",
    "$$[∇^2 f(x)]_{ij} ≈ \\frac{f(x + ε_2 e_i + ε_2 e_j) − f(x + ε_2 e_i) − f(x + ε_2 e_j) + f(x)}{ε^2_2}$$\n",
    "Здесь $ε_2 ∼\\sqrt[3]{ε_{mach}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7fe3e",
   "metadata": {},
   "source": [
    "# 3 Формулировка задания\n",
    "\n",
    "1 Скачайте коды, прилагаемые к заданию:\n",
    "\n",
    "https://github.com/arodomanov/cmc-mipt17-opt-course/tree/master/task\n",
    "\n",
    "Эти файлы содержат прототипы функций, которые Вам нужно будет реализовать. Некоторые проце-\n",
    "дуры уже частично или полностью реализованы.\n",
    "\n",
    "2 Реализовать метод градиентного спуска (функция `gradient_descent` в модуле `optimization`) и процедуру линейного поиска (метод `line_search` в классе `LineSearchTool` в модуле `optimization`).\n",
    "**Рекомендация:** Для поиска точки, удовлетворяющей сильным условиям Вульфа, воспользуйтесь биб-\n",
    "лиотечной функцией `scalar_search_wolfe2` из модуля `scipy.optimize.linesearch`. Однако следует\n",
    "иметь в виду, что у этой библиотечной функции имеется один недостаток: она иногда не сходится и\n",
    "возвращает значение `None`. Если библиотечный метод вернул `None`, то запустите процедуру дробления шага (бэктрекинг) для поиска точки, удовлетворяющей условию Армихо.\n",
    "\n",
    "3 Получить формулы для градиента и гессиана функции логистической регрессии. Выписать их в отчет\n",
    "в матрично-векторной форме с использованием поэлементных функций, но без каких-либо суммирований. Также выписать в отчетвыражение для самой функции логистической регрессии в матрично-векторной форме (без явных суммирований).  \n",
    "**Замечание:** В матрично-вектрной форме допускается использование операций матричного сложения/произведения, умножения на скаляр, транспонирования, стандартного скалярного произведения, поэлементного произведения, а также применения ко всем элементам вектора некоторой скалярной функции. Кроме этого, допускается использование стандартных матриц/векторов (заданного размера): единичная матрица $I_n$, нулевая матрица $0_{m×n}$, нулевой вектор $0_n$, вектор из всех единиц $1_n := (1,... ,1)$.\n",
    "\n",
    "4 Реализовать оракул логистической регрессии (класс `LogRegL2Oracle` в модуле `oracles`). Также доделать реализацию вспомогательной функции `create_log_reg_oracle` в модуле `oracles`.  \n",
    "**Замечание:** Реализация оракула должна быть полностью векторизованной, т. е. код не должен содержать никаких циклов.  \n",
    "**Замечание:** Ваш код должен поддерживать как плотные матрицыAтипаnp.array, так и разрежен-\n",
    "ные типа `scipy.sparse.csr_matrix`.  \n",
    "**Замечание:** Нигде в промежуточных вычислениях не стоит вычислять значение $exp(−b_i〈a_i, x〉)$, иначе может произойти переполнение. Вместо этого следует напрямую вычислять необходимые величины с помощью специализированных для этого функций: `np.logaddexp` для $ln(1+exp(·))$ и `scipy.special.expit` для $1 /(1 + exp(·))$.\n",
    "\n",
    "5 Реализовать подсчет разностных производных (функции `grad_finite_diff` и `hess_finite_diff` в модуле `oracles`). Проверить правильность реализации подсчета градиента и гессиана логистического\n",
    "оракула с помощью реализованных функций. Для этого сгенерируйте небольшую модельную выборку\n",
    "(матрицу $A$ и вектор $b$) и сравните значения, выдаваемые методами `grad` и `hess`, с соответствующими разностными аппроксимациями в нескольких пробных точкахx.\n",
    "\n",
    "6 Реализовать метод Ньютона (функция `newton` в модуле `optimization`).\n",
    "\n",
    "**Замечание:** Для поиска направления в методе Ньютона не нужно в явном виде обращать гессиан (с\n",
    "помощью функции `np.linalg.inv`) или использовать самый общий метод для решения системы линей-\n",
    "ных уравнений (`numpy.linalg.solve`). Вместо этого следует учесть тот факт, что в рассматриваемой\n",
    "задаче гессиан является симметричной положительно определенной матрицей и воспользоваться раз-\n",
    "ложением Холецкого (функции `scipy.linalg.cho_factor` и `scipy.linalg.cho_solve`).\n",
    "\n",
    "7 Провести эксперименты, описанные ниже. Написать отчет.\n",
    "\n",
    "<!-- 8 (Бонусная часть) Реализовать оптимизированный оракул логистической регрессии, который запомина-\n",
    "ет последние матрично-векторные произведения (классLogRegL2OptimizedOracleв модулеoptimization).\n",
    "Оптимизированный оракул отличается от обычного в следующих трех пунктах:\n",
    "\n",
    "1. При последовательных вычислениях значения функции (методfunc), градиента (методgrad) и\n",
    "    гессиана (методhess) в одной и той же точкеx, матрично-векторное произведениеAxне вычис-\n",
    "    ляется повторно.\n",
    "2. В процедурахfunc_directionalиgrad_directionalвыполняется предподсчет матрично-векторных\n",
    "    произведенийAxиAd. Если эти процедуры вызываются последовательно для одних и тех же зна-\n",
    "    чений точкиxи/или направленияd, то матрично-векторные произведенияAxи/илиAdзаново не\n",
    "    вычисляются. Если перед вызовом или после вызоваfunc_directionalи/илиgrad_directional\n",
    "    присутствуют вызовыfuncи/илиgradи/илиhessв той же самой точкеx, то матрично-векторное\n",
    "    произведениеAxне должно вычисляться повторно.\n",
    "3. Методыfunc_directionalиgrad_directionalзапоминают внутри себя последнюю тестовую\n",
    "    точкуxˆ:=x+αd, а также соответствующее значение матрично-векторного произведенияAxˆ=\n",
    "    Ax+αAd. Если далее одна из процедурfunc,grad,hess,func_directional,grad_directional\n",
    "    вызывается в точкеxˆ, то соответствующее матрично-векторное произведениеAˆxзаново не вы-\n",
    "    числяется.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c68f2a",
   "metadata": {},
   "source": [
    "## 3.1 Эксперимент: Траектория градиентного спуска на квадратичной функции\n",
    "\n",
    "Проанализируйте траекторию градиентного спуска для нескольких квадратичных функций: при-\n",
    "думайте две-три квадратичныедвумерныефункции, на которых работа метода будет отличаться, на-\n",
    "рисуйте графики с линиями уровня функций и траекториями методов.  \n",
    "\n",
    "Попробуйте ответить на следующий вопрос:Как отличается поведение метода в зависимости от\n",
    "числа обусловленности функции, выбора начальной точки и стратегии выбора шага (константная\n",
    "стратегия, Армихо, Вульф)?  \n",
    "\n",
    "Для рисования линий уровня можете воспользоваться функцией `plot_levels`, а для рисования\n",
    "траекторий `plot_trajectory` из файла `plot_trajectory_2d.py`, прилагающегося к заданию.  \n",
    "Также обратите внимание, что оракул квадратичной функции `QuadraticOracle` уже реализован в\n",
    "модуле `oracles`. Он реализует функцию $f(x) = (1/2)〈Ax, x〉−〈b, x〉$, где $A∈S^n_{++}, b ∈ R^n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff1b93",
   "metadata": {},
   "source": [
    "## 3.2 Эксперимент: Зависимость числа итераций градиентного спуска от числа обусловленности и размерности пространства\n",
    "\n",
    "Исследуйте, как зависит число итераций, необходимое градиентному спуску для сходимости, от сле-\n",
    "дующих двух параметров: 1) числа обусловленности $κ ≥ 1$ оптимизируемой функции и 2) размерности\n",
    "пространства $n$ оптимизируемых переменных.  \n",
    "\n",
    "Для этого для заданных параметровnиκсгенерируйте случайным образом квадратичную задачу\n",
    "размераnс числом обусловленностиκи запустите на ней градиентный спуск с некоторой фиксиро-\n",
    "ванной требуемой точностью. Замерьте число итераций $T(n,κ)$, которое потребовалось сделать методу до сходимости (успешному выходу по критерию остановки).  \n",
    "\n",
    "**Рекомендация:** Проще всего сгенерировать случайную квадратичную задачу размера $n$ с заданным числом обусловленности $κ$ следующим образом. В качестве матрицы $A∈S^n_{++}$ удобно взять просто диагональную матрицу $A= Diag(a)$, у которой диагональные элементы сгенерированы случайно\n",
    "в пределах $[1,κ]$, причем $min(a) = 1, max(a) = κ$. В качестве вектора $b∈R^n$ можно взять вектор со случайными элементами. Диагональные матрицы удобно рассматривать, поскольку с ними можно эффективно работать даже при больших значениях $n$. Рекомендуется хранить матрицу $A$ в формате разреженной диагональной матрицы (см. `scipy.sparse.diags`).  \n",
    "\n",
    "Зафиксируйте некоторое значение размерности $n$. Переберите различные числа обусловленности\n",
    "$κ$ по сетке и постройте график зависимости $T(κ,n)$ против $κ$. Поскольку каждый раз квадратичная задача генерируется случайным образом, то повторите этот эксперимент несколько раз. В результате для фиксированного значения $n$ у Вас должно получиться целое семейство кривых зависимости $T(κ,n)$ от $κ$. Нарисуйте все эти кривые одним и тем же цветом для наглядности (например, красным).  \n",
    "\n",
    "Теперь увеличьте значение $n$ и повторите эксперимент снова. Вы должны получить новое семейство\n",
    "кривых $T(n′,κ)$ против $κ$. Нарисуйте их все одним и тем же цветом, но отличным от предыдущего\n",
    "(например, синим).  \n",
    "\n",
    "Повторите эту процедуру несколько раз для других значений $n$. В итоге должно получиться несколько разных семейств кривых - часть красных (соответствующих одному значению $n$), часть синих (соответствующих другому значению $n$), часть зеленых и т. д.  \n",
    "\n",
    "Обратите внимание, что значения размерности $n$ имеет смысл перебирать по логарифмической\n",
    "сетке (например, $n = 10, n = 100, n = 1000$ и т. д.).  \n",
    "\n",
    "Какие выводы можно сделать из полученной картинки?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8d56a",
   "metadata": {},
   "source": [
    "## 3.3 Эксперимент: Сравнение методов градиентного спуска и Ньютона на реальной задаче логистической регрессии\n",
    "\n",
    "Сравнить методы градиентного спуска и Ньютона на задаче обучения логистической регрессии на\n",
    "реальных данных.\n",
    "\n",
    "В качестве реальных данных используйте следующие три набора с сайта LIBSVM [http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.)\n",
    ": *w8a*, *gisette* и *real-sim*. Коэффициент регуляризации взять стандартным образом: $λ = 1/m$.\n",
    "Параметры обоих методов взять равными параметрам по умолчанию. Начальную точку выбрать\n",
    "$x_0 = 0$.\n",
    "\n",
    "Построить графики сходимости следующих двух видов:  \n",
    "1) Зависимость значения функции от реального времени работы метода.  \n",
    "2) Зависимость относительного квадрата нормы градиента $‖∇f(x_k)‖^2_2 /‖∇f(x_0)‖^2_2$ (в логарифмической шкале) против реального времени работы.\n",
    "\n",
    "При этом оба метода (градиентный спуск и Ньютон) нужно рисовать на одном и том же графике.\n",
    "Укажите в отчете, какова стоимость итерации и сколько памяти требуется каждому из методов в\n",
    "зависимости от параметров $m$ (размер выборки) и $n$ (размерность пространства). При оценке используйте нотацию $O(·)$, скрывающую внутри себя абсолютные константы.\n",
    "\n",
    "Какие выводы можно сделать по результатам этого эксперимента? Какой из методов лучше и в\n",
    "каких ситуациях?\n",
    "\n",
    "**Рекомендация:** Любой набор данных с сайта LIBSVM представляет из себя текстовый файл в фор-\n",
    "мате svmlight. Чтобы считать такой текстовый файл, можно использовать функцию `load_svmlight_file` из модуля `sklearn.datasets`. Обратите внимание, что эта функция возвращает матрицу в формате `scipy.sparse.csr_matrix`, поэтому Ваша реализация логистического оракула должна поддерживать такие матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936edb3",
   "metadata": {},
   "source": [
    "<!-- ## 3.4 (Бонусная часть) Эксперимент: Оптимизация вычислений в градиентном спуске\n",
    "\n",
    "Сравнить градиентный спуск на логистической регрессии для обычного оракула и оптимизирован-\n",
    "ного.\n",
    "В качестве выборки использовать модельную с размерамиm= 10000,n= 8000. Пример генерации\n",
    "модельной выборки из стандартного нормального распределения:\n",
    "\n",
    "np.random.seed(31415)\n",
    "m, n = 10000, 8000\n",
    "A = np.random.randn(m, n)\n",
    "b = np.sign(np.random.randn(m))\n",
    "\n",
    "Коэффициент регуляризации выбрать стандартнымλ= 1/m.\n",
    "Параметры метода взять равными параметрам по умолчанию. Начальную точку выбратьx 0 = 0.\n",
    "Нарисовать графики:\n",
    "\n",
    "```\n",
    "(a) Зависимость значения функции от номера итерации.\n",
    "```\n",
    "```\n",
    "(b) Зависимость значения функции от реального времени работы метода.\n",
    "```\n",
    "```\n",
    "(c) Зависимость относительного квадрата нормы градиента‖∇f(xk)‖^22 /‖∇f(x 0 )‖^22 (в логарифмиче-\n",
    "ской шкале) против реального времени работы.\n",
    "```\n",
    "При этом оба метода (с обычным оракулом и с оптимизированным) нужно рисовать на одном и том\n",
    "же графике.\n",
    "Объясните, почему траектории обоих методов на первом графике совпадают.\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47aed4b",
   "metadata": {},
   "source": [
    "<!-- \n",
    "## 3.5 (Бонусная часть) Эксперимент: Стратегия выбора длины шага в градиентном спуске\n",
    "\n",
    "Исследовать, как зависит поведение метода от стратегии подбора шага: константный шаг (попро-\n",
    "бовать различные значения), бэктрэкинг (попробовать различные константыc), условия Вульфа (по-\n",
    "пробовать различные параметрыc 2 ).\n",
    "Рассмотрите квадратичную функцию и логистическую регрессию с модельными данным (сгенери-\n",
    "рованными случайно).\n",
    "Запустите для этих функций градиентный спуск с разными стратегиями выбора шагаиз одной и\n",
    "той же начальной точки.\n",
    "Нарисуйте кривые сходимости (относительная невязка по функции в логарифмической шкале про-\n",
    "тив числа итераций – для квадратичной функции, относительный квадрат нормы градиента в лога-\n",
    "рифмической шкале против числа итераций – для логистической регрессии) для разных стратегий на\n",
    "одномграфике.\n",
    "Попробуйте разные начальные точки. Ответьте на вопрос:Какая стратегия выбора шага является\n",
    "самой лучшей?\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8cefe9",
   "metadata": {},
   "source": [
    "# 4 Оформление задания\n",
    "\n",
    "Результатом выполнения задания являются  \n",
    "1) Файлы `optimization.py` и `oracles.py` с реализованными методами и оракулами.  \n",
    "2) Полные исходные коды для проведения экспериментов и рисования всех графиков. Все результаты должны быть воспроизводимыми. Если вы используете случайность - зафиксируйте `seed`.  \n",
    "3) Отчет в формате `.ipynb` о проведенных исследованиях.  \n",
    "\n",
    "Каждый проведенный эксперимент следует оформить в виде отчёта в виде одного `.ipynb` документа (название раздела - название соответствующего эксперимента). Для каждого эксперимента необходимо\n",
    "сначала написать его описание: какие функции оптимизируются, каким образом генерируются данные,\n",
    "какие методы и с какими параметрами используются. Далее должны быть представлены результаты\n",
    "соответствующего эксперимента - графики, таблицы и т. д. Наконец, после результатов эксперимента\n",
    "должны быть написаны Ваши выводы - какая зависимость наблюдается и почему.\n",
    "\n",
    "**Важно:** Отчет не должен содержать минимум кода. Каждый график должен быть прокомментирован - что на нем изображено, какие выводы можно сделать из этого эксперимента. Обязательно\n",
    "должны быть подписаны оси. Если на графике нарисовано несколько кривых, то должна быть легенда.\n",
    "Сами линии следует рисовать достаточно толстыми, чтобы они были хорошо видимыми."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae40d9",
   "metadata": {},
   "source": [
    "# 5 Проверка задания\n",
    "\n",
    "Перед отправкой задания обязательно убедитесь, что Ваша реализация проходит автоматические\n",
    "предварительныетесты `presubmit_tests.py`, выданные вместе с заданием. Для этого запустите следующую команду:\n",
    "```\n",
    ">>> nosetests3 presubmit_tests.py\n",
    "```\n",
    "\n",
    "<!-- (b) Для бонусной части (проверяются как базовые, так и бонусные тесты):\n",
    "nosetests3 presubmit_tests.py -a ’bonus’\n",
    " -->\n",
    "\n",
    "**Важно:** Решения, которые не будут проходить тесты `presubmit_tests.py`, будут автоматически\n",
    "оценены в **0 баллов**. Проверяющий не будет разбираться, почему Ваш код не работает и читать Ваш\n",
    "отчет.\n",
    "Оценка за задание будет складываться из двух частей:\n",
    "\n",
    "1) Правильность и эффективность реализованного кода.\n",
    "2) Качество отчета\n",
    "\n",
    "Правильность и эффективность реализованного кода будет оцениваться автоматически с помощью\n",
    "независимых тестов (отличных от предварительных тестов). Качество отчета будет оцениваться про-\n",
    "веряющим. При этом оценка может быть субъективной и аппеляции не подлежит.\n",
    "\n",
    "За реализацию модификаций алгоритмов и хорошие дополнительные эксперименты могут быть\n",
    "начислены дополнительные баллы. Начисление этих баллов является субъективным и безапелляцион-\n",
    "ным.\n",
    "\n",
    "**Важно:** Практическое задание выполняется самостоятельно. Если вы получили ценные советы (по\n",
    "реализации или проведению экспериментов) от другого студента, то об этом должно быть явно напи-\n",
    "сано в отчёте. В противном случае \"похожие\" решения считаются плагиатом и все задействованные\n",
    "студенты (в том числе те, у кого списали) будут сурово наказаны.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f765736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
